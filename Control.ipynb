{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Decision_Tree.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Visualization.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "#import RNN\n",
    "#import XGBOOST\n",
    "#import MLR\n",
    "#import MLoR\n",
    "import Decision_Tree\n",
    "import Visualization\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from keras import Sequential, optimizers\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Hurricane:\n",
    "    def __init__(self, ocean, id_no, year, name, length, hurr_data ):\n",
    "        self.ocean = ocean\n",
    "        self.id_no = id_no\n",
    "        self.year = year\n",
    "        self.name = name\n",
    "        self.length = length\n",
    "        self.hurr_data = hurr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_hurr_data():\n",
    "    filename = r'pickledump.txt'\n",
    "    filehandler = open(filename, 'rb') \n",
    "    new_stuff = pickle.load(filehandler)\n",
    "    return new_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_training_data(hurr_obj, columns,  hurricane_number, window_length = 5, hurricane_year = 0):\n",
    "    all_inputs = []\n",
    "    all_labels = []\n",
    "    for i in range(1):\n",
    "        if(int(hurricanes[hurricane_number].year) > hurricane_year):\n",
    "            current_df = hurricanes[hurricane_number].hurr_data\n",
    "            current_begin = 0\n",
    "            current_end = current_begin + window_length\n",
    "            while((current_end + 1) < len(current_df)):\n",
    "                hold = current_df[current_begin:current_end]\n",
    "                if(not is_negative(hold[['Max_Pressure']].values) and not is_negative(hold[['Max-wind']].values)):\n",
    "                    hold = hold[columns].values\n",
    "                    input_vals = hold\n",
    "                    labels = current_df[current_end:current_end + 1]\n",
    "                    labels = labels[columns].values\n",
    "\n",
    "                    all_inputs.append(input_vals)\n",
    "                    all_labels.append(labels)\n",
    "                current_begin += 1\n",
    "                current_end +=1\n",
    "    all_inputs = np.array(all_inputs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    return all_inputs, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def is_negative(lista):\n",
    "    #print(lista)\n",
    "    \n",
    "    #print(lista)\n",
    "    for i in range(len(lista)):\n",
    "        #print(lista[i])\n",
    "        if(int(lista[i]) < 0):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_single_lines(df, window_length, columns):\n",
    "    label_start = window_length\n",
    "    label_end = len(df)\n",
    "    input_start = 0\n",
    "    input_end = len(df) - 1\n",
    "\n",
    "    return df[label_start:label_end][columns].values, df[input_start:input_end][columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def adjust_test_results(train, predicted, actual):\n",
    "    results = []\n",
    "    for i in range(len(predicted)):\n",
    "        new_lat, new_long = add_distance(train[i][window_length - 1][0], train[i][window_length - 1][1], predicted[i][0], predicted[i][1])\n",
    "        results.append([new_lat, new_long])\n",
    "        #print(new_lat, new_long)\n",
    "        \n",
    "    results = np.array(results)\n",
    "    actual = actual[:,0,:]\n",
    "    #return results, actual\n",
    "    return mean_absolute_error(results, actual[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def adjust_training_test(train_inputs, train_labels, window_length):\n",
    "    new_labels = []\n",
    "    train_labels = train_labels[:,0,:]\n",
    "    for i in range(len(train_inputs)):\n",
    "        dist, bearing = distance(train_inputs[i][window_length - 1][0], train_inputs[i][window_length - 1][1], train_labels[i][0], train_labels[i][1])\n",
    "        new_line = [dist, bearing, train_labels[i][2], train_labels[i][3]]\n",
    "        new_labels.append(new_line)\n",
    "    return np.array(new_labels)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurricanes = load_hurr_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_training_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ed562306f7e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_train_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_train_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_test_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_train_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_training_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhurricanes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Latitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Longitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Max-wind'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Max_Pressure'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'find_training_test' is not defined"
     ]
    }
   ],
   "source": [
    "train_inputs, train_labels, test_inputs, test_labels, input_train_lines, output_train_lines, input_test_lines, input_train_lines = find_training_test(hurricanes, ['Latitude', 'Longitude', 'Max-wind', 'Max_Pressure'], 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([[0,1,2], [3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels1[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22835758410181617 41.165016459663676 0.764488066051902\n"
     ]
    }
   ],
   "source": [
    "print(mseMLR1, mseMLR2, mse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 5\n",
    "mseMLR1, model, pred2 = non_RNN_models(train_inputs, train_labels, test_inputs, test_labels, 'MLR')\n",
    "new_train_labels = adjust_training_test(train_inputs, train_labels, window_length)\n",
    "new_test_labels = adjust_training_test(test_inputs, test_labels, window_length)\n",
    "\n",
    "\n",
    "mseMLR2, model, pred2 = non_RNN_models(train_inputs, new_train_labels, test_inputs, new_test_labels, 'MLR', adjustment_needed = False)\n",
    "mse3 = adjust_test_results(test_inputs, pred2, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.2 72.5 [[  28.    73.6   30.  1010. ]] [[  28.9   74.3   30.  1007. ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs[0][window_length - 1][0], train_inputs[0][window_length - 1][1], train_labels[0], train_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def huber_approx_obj(preds, dtrain):\n",
    "\n",
    "    d = preds - dtrain.get_labels() #remove .get_labels() for sklearn\n",
    "\n",
    "    h = 1  #h is delta in the graphic\n",
    "\n",
    "    scale = 1 + (d / h) ** 2\n",
    "\n",
    "    scale_sqrt = np.sqrt(scale)\n",
    "\n",
    "    grad = d / scale_sqrt\n",
    "\n",
    "    hess = 1 / scale / scale_sqrt\n",
    "\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def non_RNN_models(train_inputs, train_labels, test_inputs, test_labels, model_type, adjustment_needed = True):\n",
    "    if(model_type == 'DT'):\n",
    "        regressor = DecisionTreeRegressor(random_state = 42)\n",
    "    elif(model_type == 'MLR'):\n",
    "        regressor = LinearRegression()\n",
    "    elif(model_type == 'XGBOOST'):\n",
    "        xgb = XGBRegressor(objective='reg:squarederror')\n",
    "        regressor = MultiOutputRegressor(xgb)\n",
    "    \n",
    "    train_inputs = (train_inputs.transpose(0,2,1).reshape(-1,20)) \n",
    "    test_inputs = (test_inputs.transpose(0,2,1).reshape(-1,20)) \n",
    "    if(adjustment_needed == True):\n",
    "        test_labels = test_labels[:,0,:]\n",
    "        train_labels = train_labels[:,0,:]\n",
    "        \n",
    "    regressor.fit(train_inputs, train_labels)\n",
    "    y_pred = regressor.predict(test_inputs)\n",
    "    MSE = mean_absolute_error(test_labels[:,0:2], y_pred[:,0:2])\n",
    "    return MSE, regressor, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def find_training_test(hurr_objs, columns, window_length, return_dim):\n",
    "    all_hurrs = []\n",
    "    for i in range(len(hurr_objs)):\n",
    "        if(hurr_objs[i].length  > window_length and not is_negative(hurr_objs[i].hurr_data[['Max_Pressure']].values) and not is_negative(hurr_objs[i].hurr_data[['Max-wind']].values)):\n",
    "            all_hurrs.append(i)\n",
    "           \n",
    "    train_hurrs, test_hurrs = train_test_split(all_hurrs,  test_size=0.20, random_state=42)\n",
    "    #if(return_dim == 2):\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    test_inputs = []\n",
    "    test_labels = []\n",
    "    all_input_train_lines = []\n",
    "    all_output_train_lines = []\n",
    "    all_input_test_lines = []\n",
    "    all_output_test_lines = []\n",
    "    print(len(hurr_objs))\n",
    "    for i in range(len(hurr_objs)):\n",
    "        print(i)\n",
    "        if(i in train_hurrs):\n",
    "            current_df = hurr_objs[i].hurr_data\n",
    "            current_begin = 0\n",
    "            current_end = current_begin + window_length\n",
    "            while((current_end + 1) < len(current_df)):\n",
    "                hold = current_df[current_begin:current_end]\n",
    "                if(not is_negative(hold[['Max_Pressure']].values) and not is_negative(hold[['Max-wind']].values)):\n",
    "                    hold = hold[columns].values\n",
    "                    input_vals = hold\n",
    "                    labels = current_df[current_end:current_end + 1]\n",
    "                    labels = labels[columns].values\n",
    "\n",
    "                    train_inputs.append(input_vals)\n",
    "                    train_labels.append(labels)\n",
    "                output_lines, input_lines = add_single_lines(current_df, window_length, columns)\n",
    "                all_input_train_lines.append(input_lines)\n",
    "                all_output_train_lines.append(output_lines)\n",
    "                current_begin += 1\n",
    "                current_end +=1\n",
    "        elif(i in test_hurrs):\n",
    "            current_df = hurr_objs[i].hurr_data\n",
    "            current_begin = 0\n",
    "            current_end = current_begin + window_length\n",
    "            while((current_end + 1) < len(current_df)):\n",
    "                hold = current_df[current_begin:current_end]\n",
    "                if(not is_negative(hold[['Max_Pressure']].values) and not is_negative(hold[['Max-wind']].values)):\n",
    "                    hold = hold[columns].values\n",
    "                    input_vals = hold\n",
    "                    labels = current_df[current_end:current_end + 1]\n",
    "                    labels = labels[columns].values\n",
    "\n",
    "                    test_inputs.append(input_vals)\n",
    "                    test_labels.append(labels)\n",
    "                    \n",
    "                output_lines, input_lines = add_single_lines(current_df, window_length, columns)\n",
    "                all_input_test_lines.append(input_lines)\n",
    "                all_output_test_lines.append(output_lines)\n",
    "                current_begin += 1\n",
    "                current_end +=1\n",
    "        \n",
    "    train_inputs = np.array(train_inputs)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_inputs = np.array(test_inputs)\n",
    "    test_labels = np.array(test_labels)\n",
    "    all_input_train_lines = np.vstack(all_input_train_lines)\n",
    "    all_output_train_lines = np.vstack(all_output_train_lines)\n",
    "    all_input_test_lines = np.vstack(all_input_test_lines)\n",
    "    all_output_test_lines = np.vstack(all_output_test_lines)\n",
    "    return train_inputs, train_labels, test_inputs, test_labels, all_input_train_lines, all_output_train_lines, all_input_test_lines, all_output_test_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_inputs, norm_test_inputs,norm_train_outputs, scalar_test, scalar_train = normalize_train(input_train_lines, input_test_lines, train_inputs, test_inputs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train(input_train_lines, input_test_lines, train_inputs, test_inputs, train_outputs):\n",
    "    values = input_train_lines\n",
    "    #values = values.reshape((len(values), 1))\n",
    "    # train the standardization\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(values[:,0:2])\n",
    "    #print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, sqrt(scaler.var_)))\n",
    "    # standardization the dataset and print the first 5 rows\n",
    "    all_norms_train = []\n",
    "    all_norms_output_train = []\n",
    "    all_norms_test = []\n",
    "    scaler_out = StandardScaler()\n",
    "    scaler_out = scaler_out.fit(input_test_lines[:,0:2])\n",
    "    for i in range(len(train_inputs)):\n",
    "        normalized_train = scaler.transform(train_inputs[i,:,0:2])\n",
    "        normalized_out_train = scaler.transform(train_outputs[i,:,0:2])\n",
    "        all_norms_train.append(normalized_train)\n",
    "        all_norms_output_train.append(normalized_out_train)\n",
    "    for i in range(len(test_inputs)):\n",
    "        normalized_test =scaler_out.transform(test_inputs[i,:,0:2])\n",
    "        all_norms_test.append(normalized_test)\n",
    "    return np.array(all_norms_train), np.array(all_norms_test), np.array(all_norms_output_train), scaler, scaler_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(5):\n",
    "\tprint(normalized[i])\n",
    "# inverse transform and print the first 5 rows\n",
    "inversed = scaler.inverse_transform(normalized)\n",
    "for i in range(5):\n",
    "\tprint(inversed[i])\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "# Standardize time series data\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "# load the dataset and print the first 5 rows\n",
    "series = read_csv('daily-minimum-temperatures-in-me.csv', header=0, index_col=0)\n",
    "print(series.head())\n",
    "# prepare data for standardization\n",
    "values = series.values\n",
    "values = values.reshape((len(values), 1))\n",
    "# train the standardization\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(values)\n",
    "print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, sqrt(scaler.var_)))\n",
    "# standardization the dataset and print the first 5 rows\n",
    "normalized = scaler.transform(values)\n",
    "for i in range(5):\n",
    "\tprint(normalized[i])\n",
    "# inverse transform and print the first 5 rows\n",
    "inversed = scaler.inverse_transform(normalized)\n",
    "for i in range(5):\n",
    "\tprint(inversed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_RNN(train_inputs, train_labels, test_inputs, test_labels, window_length, scaler_train, scaler_test, adjustment_needed = False):\n",
    "    #train_inputs = (train_inputs.transpose(0,2,1).reshape(-1,20))[:,np.newaxis, :]\n",
    "    #test_inputs = (test_inputs.transpose(0,2,1).reshape(-1,20))[:,np.newaxis, :]\n",
    "    print(train_inputs.shape)\n",
    "    print(test_inputs.shape)\n",
    "    #return train_inputs, None, None\n",
    "    if(adjustment_needed == True):\n",
    "        test_labels = test_labels[:,0,:]\n",
    "        train_labels = train_labels[:,0,:]\n",
    "    print(test_labels.shape)\n",
    "    print(train_labels.shape)\n",
    "    #return train_inputs, train_labels, test_inputs, test_labels\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=128,return_sequences = True, input_shape=(5,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(units=128,return_sequences = False, input_shape=(5,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    #model.add(Dense(units=32, activation = 'relu'))\n",
    "    #model.add(LSTM(units=128, activation = 'relu'))\n",
    "    model.add(Dense(units=2, activation = 'relu'))\n",
    "    #model.summary()\n",
    "    sgd = keras.optimizers.rmsprop(learning_rate=0.01)#, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(optimizer=sgd, loss='mean_absolute_error')\n",
    "    i = 100000000\n",
    "    model.fit(train_inputs[:,:, 0:2], train_labels[:,0:2], epochs=15, batch_size=32)\n",
    "    y_pred= model.predict(test_inputs[:,:,0:2])\n",
    "    y_pred = scaler_test.inverse_transform(y_pred)\n",
    "    # Save the model\n",
    "    model.save('rnnv2.h5')\n",
    "    mae = mean_absolute_error(test_labels[:,0:2], y_pred[:,0:2])\n",
    "\n",
    "    return mae, test_labels, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10755, 5, 2)\n",
      "(2837, 5, 2)\n",
      "(2837, 4)\n",
      "(10755, 2)\n",
      "Epoch 1/15\n",
      "10755/10755 [==============================] - 14s 1ms/step - loss: 0.5371\n",
      "Epoch 2/15\n",
      "10755/10755 [==============================] - 13s 1ms/step - loss: 0.4651\n",
      "Epoch 3/15\n",
      "10755/10755 [==============================] - 14s 1ms/step - loss: 0.4498\n",
      "Epoch 4/15\n",
      "10755/10755 [==============================] - 13s 1ms/step - loss: 0.4424\n",
      "Epoch 5/15\n",
      "10755/10755 [==============================] - 12s 1ms/step - loss: 0.4375\n",
      "Epoch 6/15\n",
      "10755/10755 [==============================] - 13s 1ms/step - loss: 0.4348: 0s - \n",
      "Epoch 7/15\n",
      "10755/10755 [==============================] - 14s 1ms/step - loss: 0.4329\n",
      "Epoch 8/15\n",
      "10755/10755 [==============================] - 16s 1ms/step - loss: 0.4310\n",
      "Epoch 9/15\n",
      "10755/10755 [==============================] - 15s 1ms/step - loss: 0.4301\n",
      "Epoch 10/15\n",
      "10755/10755 [==============================] - 13s 1ms/step - loss: 0.4280: 0s - lo\n",
      "Epoch 11/15\n",
      "10755/10755 [==============================] - 15s 1ms/step - loss: 0.4275: 0s - los\n",
      "Epoch 12/15\n",
      "10755/10755 [==============================] - 16s 1ms/step - loss: 0.4275\n",
      "Epoch 13/15\n",
      "10755/10755 [==============================] - 12s 1ms/step - loss: 0.4268\n",
      "Epoch 14/15\n",
      "10755/10755 [==============================] - 15s 1ms/step - loss: 0.4257: 0s - loss: 0.4 - ETA: 0s - loss: 0.425\n",
      "Epoch 15/15\n",
      "10755/10755 [==============================] - 16s 2ms/step - loss: 0.4250: 6s - loss: 0.4 - ETA: 6s - loss: 0.4 - ET - ETA: 3s - loss: 0.4 - E\n"
     ]
    }
   ],
   "source": [
    "#new_train_labels = adjust_training_test(train_inputs, train_labels, window_length)\n",
    "#new_test_labels = adjust_training_test(test_inputs, test_labels, window_length)\n",
    "\n",
    "mae, model, y_pred = create_RNN(norm_train_inputs, norm_train_outputs,norm_test_inputs, test_labels, window_length,scalar_train, scalar_test, adjustment_needed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10755, 1, 20)\n",
      "(2837, 1, 20)\n",
      "(2837, 4)\n",
      "(10755, 4)\n"
     ]
    }
   ],
   "source": [
    "hold1, hold2, hold3, hold4 = create_RNN(train_inputs, train_labels, test_inputs, test_labels, window_length, adjustment_needed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-7903d7b4bd1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmse3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjust_test_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-8f5d108d7c13>\u001b[0m in \u001b[0;36madjust_test_results\u001b[1;34m(train, predicted, actual)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mnew_lat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_long\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindow_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindow_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_lat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_long\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m#print(new_lat, new_long)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "mse3 = adjust_test_results((test_inputs.transpose(0,2,1).reshape(-1,20)),  y_pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864\n"
     ]
    }
   ],
   "source": [
    "hurricanes = load_hurr_data()\n",
    "print(len(hurricanes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.863678, 77.99661 ],\n",
       "       [33.603443, 77.670494],\n",
       "       [34.499813, 76.58016 ],\n",
       "       [35.084187, 75.102135],\n",
       "       [35.261593, 73.594696]], dtype=float32)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  32.4,   78.7,   25. , 1012. ]],\n",
       "\n",
       "       [[  33.3,   78. ,   25. , 1011. ]],\n",
       "\n",
       "       [[  34. ,   77. ,   30. , 1006. ]],\n",
       "\n",
       "       [[  34.4,   75.8,   35. , 1004. ]],\n",
       "\n",
       "       [[  34. ,   74.8,   40. , 1002. ]]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.840175564422722"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(model[:,0:2], y_pred[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse3 = adjust_test_results(test_inputs, model, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.506329965104001"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.23453956,  0.40932324],\n",
       "        [-0.23453956,  0.46389391],\n",
       "        [-0.22527359,  0.51350361],\n",
       "        [-0.18820969,  0.56311332],\n",
       "        [-0.12334787,  0.61272302]],\n",
       "\n",
       "       [[-0.23453956,  0.46389391],\n",
       "        [-0.22527359,  0.51350361],\n",
       "        [-0.18820969,  0.56311332],\n",
       "        [-0.12334787,  0.61272302],\n",
       "        [-0.04922008,  0.6672937 ]],\n",
       "\n",
       "       [[-0.22527359,  0.51350361],\n",
       "        [-0.18820969,  0.56311332],\n",
       "        [-0.12334787,  0.61272302],\n",
       "        [-0.04922008,  0.6672937 ],\n",
       "        [ 0.03417369,  0.70202049]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.30433738, -1.34189931],\n",
       "        [ 2.37846517, -1.47584551],\n",
       "        [ 2.48039089, -1.65444044],\n",
       "        [ 2.60084855, -1.83799634],\n",
       "        [ 2.72130622, -2.01163031]],\n",
       "\n",
       "       [[ 2.37846517, -1.47584551],\n",
       "        [ 2.48039089, -1.65444044],\n",
       "        [ 2.60084855, -1.83799634],\n",
       "        [ 2.72130622, -2.01163031],\n",
       "        [ 2.81396596, -2.13565456]],\n",
       "\n",
       "       [[ 2.48039089, -1.65444044],\n",
       "        [ 2.60084855, -1.83799634],\n",
       "        [ 2.72130622, -2.01163031],\n",
       "        [ 2.81396596, -2.13565456],\n",
       "        [ 2.89735973, -2.26463979]]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_train_inputs[:, :, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  34.,   77.,   30., 1006.])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.348331115522512"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  26. ,   26. ,   26.1, ..., 1014. , 1013. , 1012. ]],\n",
       "\n",
       "       [[  26. ,   26.1,   26.5, ..., 1013. , 1012. , 1010. ]],\n",
       "\n",
       "       [[  26.1,   26.5,   27.2, ..., 1012. , 1010. , 1007. ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  53.4,   54.2,   55.3, ...,  952. ,  956. ,  960. ]],\n",
       "\n",
       "       [[  54.2,   55.3,   56.6, ...,  956. ,  960. ,  964. ]],\n",
       "\n",
       "       [[  55.3,   56.6,   57.9, ...,  960. ,  964. ,  968. ]]])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def distance(s_lat, s_lng, e_lat, e_lng):\n",
    "\n",
    "   # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    s_lat = s_lat*np.pi/180.0                      \n",
    "    s_lng = np.deg2rad(s_lng)     \n",
    "    e_lat = np.deg2rad(e_lat)                       \n",
    "    e_lng = np.deg2rad(e_lng)  \n",
    "\n",
    "    d = np.sin((e_lat - s_lat)/2)**2 + np.cos(s_lat)*np.cos(e_lat) * np.sin((e_lng - s_lng)/2)**2\n",
    "    \n",
    "    bearing = get_bearing(s_lat, s_lng, e_lat, e_lng)\n",
    "\n",
    "    return 2 * R * np.arcsin(np.sqrt(d)), bearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_bearing(lat1, long1, lat2, long2):\n",
    "    dLon = (long2 - long1)\n",
    "\n",
    "    y = math.sin(dLon) * math.cos(lat2)\n",
    "    x = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dLon)\n",
    "\n",
    "    brng = math.atan2(y, x)\n",
    "\n",
    "    brng = np.rad2deg(brng)\n",
    "\n",
    "    return brng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_distance( lat, lon, bearing, distance):\n",
    "    EARTH_RADIUS = 6371.00\n",
    "    # convert Latitude and Longitude\n",
    "    # into radians for calculation\n",
    "    latitude = math.radians(lat)\n",
    "    longitute = math.radians(lon)\n",
    "\n",
    "    # calculate next latitude\n",
    "    next_latitude = math.asin(math.sin(latitude) *\n",
    "                    math.cos(distance/EARTH_RADIUS) +\n",
    "                    math.cos(latitude) *\n",
    "                    math.sin(distance/EARTH_RADIUS) *\n",
    "                    math.cos(math.radians(bearing)))\n",
    "\n",
    "    # calculate next longitude\n",
    "    next_longitude = longitute + (math.atan2(math.sin(math.radians(bearing)) *\n",
    "                                             math.sin(distance/EARTH_RADIUS) *\n",
    "                                             math.cos(latitude),\n",
    "                                             math.cos(distance/EARTH_RADIUS) -\n",
    "                                             math.sin(latitude) *\n",
    "                                             math.sin(next_latitude)\n",
    "                                            )\n",
    "                                 )\n",
    "\n",
    "    # convert points into decimal degrees\n",
    "    new_lat = math.degrees(next_latitude)\n",
    "    new_lon = math.degrees(next_longitude)\n",
    "\n",
    "    return new_lat, new_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0,
     11,
     22
    ]
   },
   "outputs": [],
   "source": [
    "def create_map_input_MLR(df_hurr, hurr_num):\n",
    "    inputs, labels = create_training_data(hurricanes, ['Latitude', 'Longitude', 'Max-wind', 'Max_Pressure'], 1700)\n",
    "    actual_pred, y_pred, MSE = MLR_create_and_pred(inputs)\n",
    "    output = hold\n",
    "    df_hurr = df_hurr[hurr_num].hurr_data\n",
    "    df_hurr = df_hurr[['Latitude', 'Longitude']]\n",
    "    df_output = pd.DataFrame({\n",
    "        'Latitude':output[:, 0],\n",
    "        'Longitude':output[:, 1]\n",
    "    })\n",
    "    Visualization.plot_hurricanes(df_output, df_hurr)\n",
    "def create_map_input_XGBOOST(df_hurr, hurr_num):\n",
    "    inputs, labels = create_training_data(hurricanes, ['Latitude', 'Longitude', 'Max-wind', 'Max_Pressure'], 1700)\n",
    "    hold = XGBOOST.load_and_predict(1,inputs)\n",
    "    output = hold\n",
    "    df_hurr = df_hurr[hurr_num].hurr_data\n",
    "    df_hurr = df_hurr[['Latitude', 'Longitude']]\n",
    "    df_output = pd.DataFrame({\n",
    "        'Latitude':output[:, 0],\n",
    "        'Longitude':output[:, 1]\n",
    "    })\n",
    "    Visualization.plot_hurricanes(df_output, df_hurr)\n",
    "def create_map_input_Decision(df_hurr, hurr_num):\n",
    "    inputs, labels = create_training_data(hurricanes, ['Latitude', 'Longitude', 'Max-wind', 'Max_Pressure'], 1700)\n",
    "    hold, hold1, hold2 = Decision_Tree.create_and_pred(inputs)\n",
    "    output = hold\n",
    "    df_hurr = df_hurr[hurr_num].hurr_data\n",
    "    df_hurr = df_hurr[['Latitude', 'Longitude']]\n",
    "    df_output = pd.DataFrame({\n",
    "        'Latitude':output[:, 0],\n",
    "        'Longitude':output[:, 1]\n",
    "    })\n",
    "    Visualization.plot_hurricanes(df_output, df_hurr)\n",
    "    return hold    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create_map_input_MLoR(hurricanes, 1700)\n",
    "#create_map_input_MLR(hurricanes, 1700)\n",
    "#create_map_input_XGBOOST(hurricanes, 1700)\n",
    "#hold = create_map_input_Decision(hurricanes, 1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '1970-present_OFCL_v_BCD5_ind_ATL_TI_errors_noTDs.txt.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df4 = pd.read_csv(filename, delimiter='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date/Time</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>000hT01</th>\n",
       "      <th>012hT01</th>\n",
       "      <th>024hT01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8163</th>\n",
       "      <td>25-09-2008/06:00:00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>69.4</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8164</th>\n",
       "      <td>25-09-2008/12:00:00</td>\n",
       "      <td>22.4</td>\n",
       "      <td>68.7</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8165</th>\n",
       "      <td>25-09-2008/18:00:00</td>\n",
       "      <td>23.1</td>\n",
       "      <td>68.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8166</th>\n",
       "      <td>26-09-2008/00:00:00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40.3</td>\n",
       "      <td>30.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167</th>\n",
       "      <td>26-09-2008/06:00:00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>68.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8168</th>\n",
       "      <td>26-09-2008/12:00:00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>68.6</td>\n",
       "      <td>5.4</td>\n",
       "      <td>42.5</td>\n",
       "      <td>36.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8169</th>\n",
       "      <td>26-09-2008/18:00:00</td>\n",
       "      <td>26.9</td>\n",
       "      <td>68.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>43.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8170</th>\n",
       "      <td>27-09-2008/00:00:00</td>\n",
       "      <td>28.7</td>\n",
       "      <td>68.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8171</th>\n",
       "      <td>27-09-2008/06:00:00</td>\n",
       "      <td>29.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>7.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8172</th>\n",
       "      <td>27-09-2008/12:00:00</td>\n",
       "      <td>31.2</td>\n",
       "      <td>69.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>48.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173</th>\n",
       "      <td>27-09-2008/18:00:00</td>\n",
       "      <td>33.3</td>\n",
       "      <td>69.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>69.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8174</th>\n",
       "      <td>28-09-2008/00:00:00</td>\n",
       "      <td>35.3</td>\n",
       "      <td>69.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>43.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8175</th>\n",
       "      <td>28-09-2008/06:00:00</td>\n",
       "      <td>37.4</td>\n",
       "      <td>69.3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>67.6</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8176</th>\n",
       "      <td>28-09-2008/12:00:00</td>\n",
       "      <td>39.4</td>\n",
       "      <td>68.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8177</th>\n",
       "      <td>28-09-2008/18:00:00</td>\n",
       "      <td>41.6</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>29-09-2008/00:00:00</td>\n",
       "      <td>43.8</td>\n",
       "      <td>66.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Date/Time   Lat   Lon  000hT01  012hT01  024hT01\n",
       "8163  25-09-2008/06:00:00  22.0  69.4  -9999.0  -9999.0  -9999.0\n",
       "8164  25-09-2008/12:00:00  22.4  68.7  -9999.0  -9999.0  -9999.0\n",
       "8165  25-09-2008/18:00:00  23.1  68.4      0.0     13.2     48.3\n",
       "8166  26-09-2008/00:00:00  24.0  68.0     12.0     40.3     30.5\n",
       "8167  26-09-2008/06:00:00  25.0  68.2      0.0     37.5     37.5\n",
       "8168  26-09-2008/12:00:00  26.0  68.6      5.4     42.5     36.3\n",
       "8169  26-09-2008/18:00:00  26.9  68.6      0.0     24.0     43.2\n",
       "8170  27-09-2008/00:00:00  28.7  68.6      0.0     31.3     45.6\n",
       "8171  27-09-2008/06:00:00  29.9  69.3      7.9     18.0     36.3\n",
       "8172  27-09-2008/12:00:00  31.2  69.5      0.0     36.3     48.6\n",
       "8173  27-09-2008/18:00:00  33.3  69.7      0.0     15.3     69.8\n",
       "8174  28-09-2008/00:00:00  35.3  69.7      0.0     28.4     43.8\n",
       "8175  28-09-2008/06:00:00  37.4  69.3      4.8     67.6  -9999.0\n",
       "8176  28-09-2008/12:00:00  39.4  68.2      4.6     30.3  -9999.0\n",
       "8177  28-09-2008/18:00:00  41.6  66.7      0.0  -9999.0  -9999.0\n",
       "8178  29-09-2008/00:00:00  43.8  66.2      0.0  -9999.0  -9999.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df4[df4['STMID'] == 'AL112008'][['Date/Time','Lat', 'Lon','000hT01','012hT01', '024hT01']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
